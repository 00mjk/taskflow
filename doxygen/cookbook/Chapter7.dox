namespace tf {

/** @page chapter7 GPU Tasking (%cudaFlowCapturer)

You can create a %cudaFlow through <i>stream capture</i>, which allows you
to capture information on GPU activities that are submitted to the stream 
managed by a @em %cudaFlowCapturer.

[TOC]

@section C7_Capture_a_cudaFlow Capture a cudaFlow

When your program has no access to direct kernel calls but invoke
it through a stream-based interface (e.g., @cuBLAS and @cuDNN library functions),
you can use tf::cudaFlowCapturer to capture the GPU activities into a %cudaFlow.
A %cudaFlowCapturer is similar to a %cudaFlow except it forms a GPU task graph
through <i>stream capture</i>.
You use the method tf::cudaFlowCapturer::on
to capture a sequence of @em asynchronous CUDA operations through the given stream.

The following example creates a CUDA graph that captures two kernel tasks, 
@c task_1 and @c task_2, where @c task_1 (i.e., @c my_kernel_1) 
runs before @c task_2 (i.e., @c my_kernel_2).

@code{.cpp}
tf::Task task = taskflow.emplace([&](tf::cudaFlowCapturer& capturer){
  // capture my_kernel_1 through a stream managed by capturer
  tf::cudaTask task_1 = capturer.on([&](cudaStream_t stream){ 
    my_kernel_1<<<grid_1, block_1, shm_size_1, stream>>>(my_parameters_1);
  });

  // capture my_kernel_2 through a stream managed by capturer
  tf::cudaTask task_2 = capturer.on([&](cudaStream_t stream){ 
    my_kernel_2<<<grid_2, block_2, shm_size_2, stream>>>(my_parameters_2);
  });

  // my_kernel_1 runs before my_kernel_2
  task_1.precede(task_2);
});
@endcode

The stream object passed to each tf::cudaFlowCapturer::on call may differ,
and it depends on how the internal optimization algorithm maximizes the 
GPU parallelism.

@warning
Inside tf::cudaFlowCapturer::on, you should @em NOT modify the properties of 
the stream argument but use it to capture @em asynchronous GPU operations
(e.g., @c kernel, @c cudaMemcpyAsync).

A %cudaFlowCapturer lives with the callable.
When the executor invoke the capturer callable, it creates the %cudaFlowCapturer
and will destroy it until all internal operations finish.

@section C7_CreateACapturerWithinAcudaFlow Create a Capturer within a cudaFlow

Within a parent %cudaFlow, you can capture a %cudaFlow to form a subflow that 
eventually becomes a @em child node in the underlying CUDA task graph.
The following example defines a captured flow @c task2 of two dependent tasks,
@c task2_1 and @c task2_2, and @c task2 runs after @c task1.

@code{.cpp}
tf::Task task = taskflow.emplace([&](tf::cudaFlow& cf){

  tf::cudaTask task1 = cf.kernel(grid, block, shm, my_kernel, args...);
  
  // task2 forms a child node in cf (and the underlying CUDA graph)
  tf::cudaTask task2 = cf.capture([&](tf::cudaFlowCapturer& capturer){
    tf::cudaTask task2_1 = capturer.on([&](cudaStream_t stream){  
      my_kernel2<<<grid1, block1, shm_size1, stream>>>(args1...);
    });
    tf::cudaTask task2_2 = capturer.on([&](cudaStream_t stream){  
      my_kernel2<<<grid2, block2, shm_size2, stream>>>(args2...);
    });
    task2_1.precede(task2_2);
  });

  task1.precede(task2);
});
@endcode

@section C7_CommonCaptureMethods Common Capture Methods

A %cudaFlowCapturer inherits tf::cudaFlowCapturerBase that defines a set of 
methods for capturing common GPU operations,
including tf::cudaFlowCapturerBase::kernel, tf::cudaFlowCapturerBase::memcpy,
and tf::cudaFlowCapturerBase::memset.
For example, the following code snippet constructs a GPU task graph of
one host-to-device copy, kernel, and one device-to-host copy,
in this order of their dependencies.

@code{.cpp}
tf::Task task = taskflow.emplace([](tf::cudaFlowCapturer& capturer){
  // copy data from host_data to gpu_data
  tf::cudaTask h2d = capturer.memcpy(gpu_data, host_data, bytes);

  // capture my_kernel to do computation on gpu_data
  tf::cudaTask kernel = capturer.on([&](cudaStream_t stream){  
    my_kernel<<<grid, block, shm_size, stream>>>(gpu_data, arg1, arg2, ...);
  });

  // copy data from gpu_data to host_data
  tf::cudaTask d2h = capturer.memcpy(host_data, gpu_data, bytes);
  
  h2d.precede(kernel);
  kernel.precede(d2h);
});
@endcode

@section C7_CreateACapturerOnASpecificGPU Create a Capturer on a Specific GPU

You can capture a %cudaFlow on a specific GPU by calling tf::Taskflow::emplace_on.
By default, a %cudaFlow runs on the current GPU associated with the caller, 
which is typically 0.
Similar to @ref C6_run_a_cudaflow_on_multiple_gpus,
you can emplace a %cudaFlowCapturer on a specific GPU.

@code{.cpp}
tf::Task task = taskflow.emplace_on([](tf::cudaFlowCapturer& capturer){
  // here, capturer is under GPU device context 2
  // ...
}, 2);
@endcode

The above example creates a capturer on GPU 2. 
When the executor runs the callable, it switches to GPU 2 
and all the functions within the callable are called under this context.
Keep in mind, %Taskflow does not deal with memory.
It is users' responsibility to ensure the allocated memory to sit in the right context.
In this example, the GPU memory you created must come from context 2 as well.
You may use unified shared memory (i.e., @c cudaMallocManaged) to avoid this pitfall,
given that the extra cost of automatic memory migration is 
not hurting your application performance.


@section C7_CreateCustomCapturer Create a Custom Capturer

By inheriting tf::cudaFlowCapturerBase, you can create your own capturer.
The tf::cudaFlowCapturer has a factory interface, tf::cudaFlowCapturer::make_capturer,
for users to create custom capturers with lifetimes managed by the capturer.
This is convenient when you need certain objects alive during the capturing.

@code{.cpp}
class MyCapturer : public tf::cudaFlowCapturerBase {

  public:

  MyCapturer(args_to_construct_MyCapturer) {
    // ...
  }

  tf::cudaTask matrix_multiplication(args...) {
    return on([this, args...](cudaStream_t stream){
      cublasSetStream(_handle, stream);
      cublasSgemm(_handle, args...);
    });
  }

  private:

    cuBlasHandle_t _handle;
    Data _other_internal_data;
};

tf::Task task = taskflow.emplace([&](tf::cudaFlowCapturer& capturer){

  MyCapturer* mc = capturer.make_capturer<MyCapturer>(args_to_MyCapturer);

  tf::cudaTask task_1 = mc->matrix_multiplication(args...);
  tf::cudaTask task_2 = capturer.on([&](cudaStream_t stream){
    other_kernel<<<grid, block, shm_size, stream>>>(other_args...);
  });

  task_1.precede(task_2);
});
@endcode

The above example shows a custom capturer on top of @cuBLAS that
includes the @c cublasHandle object @c _handle to associate operations with streams.

*/

}


