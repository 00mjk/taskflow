namespace tf {

/** @page LinearAlgebracublasFlowCapturer Linear Algebra (%cublasFlowCapturer)

%Taskflow provides a library tf::cublasFlowCapturer to program and accelerate 
<i>basic linear algebra subprograms</i>
(BLAS) on top of @cuBLAS.

[TOC]

@image html images/LinearAlgebra.png width=50%

@section WhatIsBLAS What is BLAS?

The BLAS (Basic Linear Algebra Subprograms) are routines that provide standard 
building blocks for performing basic vector and matrix operations. 
There are three levels:

  1. Level 1: performs scalar, vector, and vector-vector operations
  2. Level 2: performs matrix-vector operations
  3. Level 3: performs matrix-matrix operations

BLAS is commonly used by linear algebra software. 
The @cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) 
on top of the Nvidia CUDA runtime
and it allows users to access the computational resources of Nvidia GPUs.

@section HowToUsecublasFlowCapturer What is a cublasFlow Capturer? Why?

tf::cublasFlowCapturer provides an interface over native cuBLAS functions
and allows users to express linear algebra algorithms using a <i>task graph model</i>.
We transform the task graph into a CUDA graph using a stream capture algorithm
optimized for maximum concurrency.
When a cuBLAS program is transformed into a CUDA graph,
we can launch the entire graph using a single kernel call to largely reduce
the kernel call overheads and efficiently overlap concurrent kernels.
This organization is particularly suitable for large linear algebra programs
that contain hundreds of cuBLAS operations.

The following example use tf::cublasFlowCapturer to perform a 2-norm operation
on a vector.

@code{.cpp}
const int N = 1024;

tf::Executor executor;
tf::Taskflow taskflow("cublas 2-norm");          // initialize an unit vector

std::vector<float> hvec(N, 1); 
float  hres;                                     // cpu vector
float* gvec = tf::cuda_malloc_device<float>(N);  // gpu vector
float* gres = tf::cuda_malloc_device<float>(1);  // gpu result

taskflow.emplace([&](tf::cudaFlowCapturer& capturer){
  // create a cuBLAS flow capturer
  auto blas = capturer.make_capturer<tf::cublasFlowCapturer>();
  
  tf::cudaTask h2d = capturer.copy(gvec, hvec.data(), N).name("h2d");
  tf::cudaTask nrm = blas->nrm2(N, gvec, 1, gres).name("2-norm");
  tf::cudaTask d2h = capturer.copy(&hres, gres, 1).name("d2h");

  nrm.precede(d2h)
     .succeed(h2d);
}).name("capturer");

executor.run(taskflow).wait();

taskflow.dump(std::cout);

assert(hres == 32);
@endcode

@dotfile images/cublas_flow_capturer_norm2.dot

@section DataModelOscublasFlowCapturer Understand the Data Model

The data pointers used within tf::cublasFlowCapturer must sit
in GPU memory space, including scalar pointers (@c alpha and @c beta),
input pointers (e.g., vectors, matrices), and output pointers (e.g., result).
By default, we set the pointer mode to @c CUBLAS_POINTER_MODE_DEVICE.
You must allocate required matrices and vectors in the GPU memory space,
fill them with data, call the methods defined in tf::cublasFlowCapturer,
and then upload the results from GPU memory space back to the host.

@note tf::cublasFlowCapturer currently supports only @c float 
and @c double data types.

The cuBLAS library uses <i>column-major storage</i> and 1-based indexing.
Since C/C++ adopts row-major storage, we cannot use the native array semantics
when matrix-matrix or matrix-vector operations are involved.
We often need extra transposition on input matrices before these operations
can take place correctly.
In terms of storage, 
a row-major matrix is equivalent to a transposed column-major matrix,
as shown below:

@f[
A_{RowMajor} \iff A^T_{ColumnMajor}
@f]

@subsection cublasFlowCapturerDataModelExample Example

Suppose we have a method <tt>matmul(A, B, C)</tt> that multiplies two matrices
@c A and @c B and stores the result in @c C,
using column-major storage.
In C/C++, data layout is mostly row-major.
Since we know a row-major matrix is equivalent in storage 
to a transposed column-major matrix,
we can take a transposed view of this multiplication:

@f[
C^T = B^T \times A^T
@f]

If the given matrices @c A, @c B, and @c C are in row-major storage,
calling <tt>matmul(A, B, C)</tt> is equivalent to the above transposed version.
The function stores the result of transposed @c C in column-major storage
which in turns translates to row-major layout of @c C -- 
<i>our desired solution</i>.

@section cublasFlowCapturerLevel-1 Use Level-1 Methods

We currently support the following level-1 methods:

  + tf::cublasFlowCapturer::amax finds the smallest element index of 
  the maximum absolute magnitude over a vector
  + tf::cublasFlowCapturer::amin finds the smallest element index of
  the minimum absolute magnitude over a vector 
  + tf::cublasFlowCapturer::asum computes the sum of absolute values of 
  elements over a vector
  + tf::cublasFlowCapturer::axpy multiplies a vector by a scalar and adds
  it to another vector
  + tf::cublasFlowCapturer::copy copies a vector into another vector
  + tf::cublasFlowCapturer::dot computes the dot product of two vectors
  + tf::cublasFlowCapturer::nrm2 computes the Euclidean norm of a vector
  + tf::cublasFlowCapturer::scal multiples a vector by a scalar
  + tf::cublasFlowCapturer::swap interchanges the elements of two vectors

Our level-1 methods capture the native <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-1-function-reference">cublas level-1 calls</a>
with internal stream(s) optimized for maximum concurrency.

@section cublasFlowCapturerLeve2-1 Use Level-2 Methods

We currently support the following level-2 methods:
  
  + tf::cublasFlowCapturer::gemv performs general matrix-vector multiplication
  + tf::cublasFlowCapturer::c_gemv performs general matrix-vector multiplication
    on C-styled row-major layout

Our level-2 methods capture the native <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-2-function-reference">cublas level-2 calls</a>
with internal stream(s) optimized for maximum concurrency.

@section cublasFlowCapturerLevel-3 Use Level-3 Methods

We currently support the following level-3 methods:
  
  + tf::cublasFlowCapturer::geam performs matrix-matrix addition/transposition
  + tf::cublasFlowCapturer::c_geam performs matrix-matrix addition/transposition 
    on C-styled row-major storage
  + tf::cublasFlowCapturer::gemm performs general matrix-matrix multiplication
  + tf::cublasFlowCapturer::c_gemm performs general matrix-matrix multiplication 
    on C-styled row-major storage
  + tf::cublasFlowCapturer::gemm_batched performs batched general 
    matrix-matrix multiplication 
  + tf::cublasFlowCapturer::c_gemm_batched performs batched general 
    matrix-matrix multiplication on C-styled row-major storage
  + tf::cublasFlowCapturer::gemm_sbatched performs batched general 
    matrix-matrix multiplication with strided memory access
  + tf::cublasFlowCapturer::c_gemm_sbatched performs batched general 
    matrix-matrix multiplication with strided memory access on C-styled row-major storage

Our level-3 methods capture the native <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-2-function-reference">cublas level-3 calls</a> and
<a href="https://docs.nvidia.com/cuda/cublas/index.html#blas-like-extension">cublas-extension calls</a>
with internal stream(s) optimized for maximum concurrency.

@section cublasFlowCapturerExtension Extend to Other cuBLAS Methods

tf::cublasFlowCapturer is derived from tf::cudaFlowCapturerBase
and is created from a factory interface tf::cudaFlowCapturer::make_capturer.
You can use all the base methods tf::cudaFlowCapturerBase::on to capture
other cuBLAS methods that are not defined in tf::cudaFlowCapturer.
The following example captures the Hermitian rank-k update,
@c cublasCherkx.

@code{.cpp}
taskflow.emplace([&](tf::cudaFlowCapturer& capturer){
  // create a cublasFlow capturer
  auto blas = capturer.make_capturer<tf::cublasFlowCapturer>();

  // use the base method tf::cudaFlowCapturer::on to capture other functions
  blas->on([&](cudaStream_t stream){
    cublasSetStream(blas->native_handle(), stream);
    cublasCherkx(blas->native_handle(), your_args...);
  }).name("Hermitian rank-k update");

}).name("capturer");
@endcode

@section cublasFlowCapturerKnowMore Know More About cublasFlow Capturer

We summarize below resources for you to know more about 
tf::cublasFlowCapturer:
  + Study the reference of tf::cublasFlowCapturer and tf::cudaFlowCapturer
  + Contribute to cublas_flow.hpp by adding more BLAS methods
  + See the complete list of BLAS functions offered by @cuBLAS



*/
}






